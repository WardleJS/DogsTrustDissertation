{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"mount_file_id":"18mNFHp7G7RM5OkdOamqSSzahzyni6s8u","authorship_tag":"ABX9TyPPRQTi4B90LDN+tF1tht7g"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["Pip Install for model use purposes\n"],"metadata":{"id":"gWQXHvxmdtI5"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Ug6ZQh8v9-nQ","executionInfo":{"status":"ok","timestamp":1720496742752,"user_tz":-60,"elapsed":87690,"user":{"displayName":"Jack Wardle","userId":"17218716300059501347"}},"outputId":"1550c851-0959-43d0-c932-fff1a2a52bb5"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting ultralytics\n","  Downloading ultralytics-8.2.51-py3-none-any.whl (799 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m799.6/799.6 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy<2.0.0,>=1.23.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (1.25.2)\n","Requirement already satisfied: matplotlib>=3.3.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (3.7.1)\n","Requirement already satisfied: opencv-python>=4.6.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (4.8.0.76)\n","Requirement already satisfied: pillow>=7.1.2 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (9.4.0)\n","Requirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (6.0.1)\n","Requirement already satisfied: requests>=2.23.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (2.31.0)\n","Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (1.11.4)\n","Requirement already satisfied: torch>=1.8.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (2.3.0+cu121)\n","Requirement already satisfied: torchvision>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (0.18.0+cu121)\n","Requirement already satisfied: tqdm>=4.64.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (4.66.4)\n","Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from ultralytics) (5.9.5)\n","Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.10/dist-packages (from ultralytics) (9.0.0)\n","Requirement already satisfied: pandas>=1.1.4 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (2.0.3)\n","Requirement already satisfied: seaborn>=0.11.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (0.13.1)\n","Collecting ultralytics-thop>=2.0.0 (from ultralytics)\n","  Downloading ultralytics_thop-2.0.0-py3-none-any.whl (25 kB)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.2.1)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (4.53.0)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.4.5)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (24.1)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (3.1.2)\n","Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.4->ultralytics) (2023.4)\n","Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.4->ultralytics) (2024.1)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (3.7)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (2024.6.2)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (3.15.4)\n","Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (4.12.2)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (1.12.1)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (3.3)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (3.1.4)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (2023.6.0)\n","Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch>=1.8.0->ultralytics)\n","  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n","Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch>=1.8.0->ultralytics)\n","  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n","Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch>=1.8.0->ultralytics)\n","  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n","Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch>=1.8.0->ultralytics)\n","  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n","Collecting nvidia-cublas-cu12==12.1.3.1 (from torch>=1.8.0->ultralytics)\n","  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n","Collecting nvidia-cufft-cu12==11.0.2.54 (from torch>=1.8.0->ultralytics)\n","  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n","Collecting nvidia-curand-cu12==10.3.2.106 (from torch>=1.8.0->ultralytics)\n","  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n","Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch>=1.8.0->ultralytics)\n","  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n","Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch>=1.8.0->ultralytics)\n","  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n","Collecting nvidia-nccl-cu12==2.20.5 (from torch>=1.8.0->ultralytics)\n","  Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n","Collecting nvidia-nvtx-cu12==12.1.105 (from torch>=1.8.0->ultralytics)\n","  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n","Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (2.3.0)\n","Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.8.0->ultralytics)\n","  Downloading nvidia_nvjitlink_cu12-12.5.82-py3-none-manylinux2014_x86_64.whl (21.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m42.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib>=3.3.0->ultralytics) (1.16.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.8.0->ultralytics) (2.1.5)\n","Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.8.0->ultralytics) (1.3.0)\n","Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, ultralytics-thop, ultralytics\n","Successfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.5.82 nvidia-nvtx-cu12-12.1.105 ultralytics-8.2.51 ultralytics-thop-2.0.0\n"]}],"source":["pip install ultralytics\n"]},{"cell_type":"code","source":["from ultralytics import YOLO\n","model = YOLO(\"yolov8s.pt\")\n","results = model.train(data=\"coco128.yaml\", epochs=10)"],"metadata":{"id":"tRCMEBys-pun"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["results = model(\"./1145217_vincent 6.jpg\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mfRVhK5xT0Bk","executionInfo":{"status":"ok","timestamp":1718799182519,"user_tz":-60,"elapsed":808,"user":{"displayName":"Jack Wardle","userId":"17218716300059501347"}},"outputId":"10523b32-0ff8-426a-d3a2-ac8c953eefb3"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","image 1/1 /content/1145217_vincent 6.jpg: 448x640 1 person, 1 dog, 521.2ms\n","Speed: 5.2ms preprocess, 521.2ms inference, 1.8ms postprocess per image at shape (1, 3, 448, 640)\n"]}]},{"cell_type":"markdown","source":["Mount to google drive to access images"],"metadata":{"id":"Ks_B1lU9d8KC"}},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"jKG2h_sj2GlH","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1720475674912,"user_tz":-60,"elapsed":1862,"user":{"displayName":"Jack Wardle","userId":"17218716300059501347"}},"outputId":"8f1f634b-51dc-4cfc-bfb5-925fa5edf406"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"markdown","source":["Use the Yolo model to label the image content, and save these labels to a csv file.The annotations are then combine in Excel with the other Data."],"metadata":{"id":"RcXj3dd-eBbU"}},{"cell_type":"code","source":["import os\n","import csv\n","from ultralytics import YOLO\n","\n","model = YOLO(\"yolov8s.pt\")\n","\n","folder_path = '/content/drive/MyDrive/images'\n","\n","results = []\n","\n","# Process each image in the folder\n","for image_name in os.listdir(folder_path):\n","    image_path = os.path.join(folder_path, image_name)\n","    if os.path.isfile(image_path):\n","        # Perform detection on the image\n","        try:\n","            detection_results = model(image_path)\n","\n","            # Extract detected objects\n","            detected_objects = []\n","            for result in detection_results:\n","                for label in result.boxes.cls:\n","                    detected_objects.append(model.names[int(label)])\n","\n","            # Append the results\n","            results.append([image_name, \", \".join(detected_objects)])\n","        except Exception as e:\n","            print(f\"Error processing {image_name}: {e}\")\n","\n","csv_file_path = 'image_labels_10.csv'\n","with open(csv_file_path, mode='w', newline='') as file:\n","    writer = csv.writer(file)\n","    writer.writerow(['ImageName', 'DetectedObjects'])\n","    writer.writerows(results)\n","\n","print(f\"Results saved to {csv_file_path}\")\n"],"metadata":{"id":"Whnk6NRY2C1A","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1720527308358,"user_tz":-60,"elapsed":78820,"user":{"displayName":"Jack Wardle","userId":"17218716300059501347"}},"outputId":"10ef32a1-7719-4415-b728-fd8eae2978b1"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","image 1/1 /content/drive/MyDrive/images/1079082_max 1.jpg: 480x640 1 dog, 496.5ms\n","Speed: 4.3ms preprocess, 496.5ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n","\n","image 1/1 /content/drive/MyDrive/images/1086931_IMG_2417.jpg: 480x640 1 person, 1 dog, 1 bowl, 795.3ms\n","Speed: 85.5ms preprocess, 795.3ms inference, 2.4ms postprocess per image at shape (1, 3, 480, 640)\n","\n","image 1/1 /content/drive/MyDrive/images/1090989_izzie 2.jpg: 448x640 1 person, 1 dog, 562.5ms\n","Speed: 6.4ms preprocess, 562.5ms inference, 1.9ms postprocess per image at shape (1, 3, 448, 640)\n","\n","image 1/1 /content/drive/MyDrive/images/1079082_max c.jpg: 480x640 1 dog, 563.3ms\n","Speed: 5.3ms preprocess, 563.3ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n","\n","image 1/1 /content/drive/MyDrive/images/1097654_Misty 654 6.jpg: 448x640 1 dog, 1 sports ball, 673.1ms\n","Speed: 4.8ms preprocess, 673.1ms inference, 3.0ms postprocess per image at shape (1, 3, 448, 640)\n","\n","image 1/1 /content/drive/MyDrive/images/1094849_Bonnie3.jpeg: 640x480 1 dog, 776.6ms\n","Speed: 6.0ms preprocess, 776.6ms inference, 1.8ms postprocess per image at shape (1, 3, 640, 480)\n","\n","image 1/1 /content/drive/MyDrive/images/1094842_Baby Roxy 842 3.jpg: 448x640 1 dog, 755.1ms\n","Speed: 4.8ms preprocess, 755.1ms inference, 2.2ms postprocess per image at shape (1, 3, 448, 640)\n","\n","image 1/1 /content/drive/MyDrive/images/1098560_oscar beach 31 (Small).jpg: 480x640 2 persons, 1 dog, 807.0ms\n","Speed: 2.8ms preprocess, 807.0ms inference, 2.3ms postprocess per image at shape (1, 3, 480, 640)\n","\n","image 1/1 /content/drive/MyDrive/images/1100892_sully at the beach.jpg: 480x640 1 person, 1 dog, 870.4ms\n","Speed: 5.8ms preprocess, 870.4ms inference, 2.3ms postprocess per image at shape (1, 3, 480, 640)\n","\n","image 1/1 /content/drive/MyDrive/images/1100414_Bella 414 3.jpg: 448x640 1 dog, 799.1ms\n","Speed: 5.5ms preprocess, 799.1ms inference, 1.7ms postprocess per image at shape (1, 3, 448, 640)\n","\n","image 1/1 /content/drive/MyDrive/images/1103882_alpha a.jpg: 480x640 1 dog, 752.9ms\n","Speed: 4.8ms preprocess, 752.9ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n","\n","image 1/1 /content/drive/MyDrive/images/1105039_run2.jpg: 448x640 1 dog, 627.3ms\n","Speed: 5.2ms preprocess, 627.3ms inference, 1.7ms postprocess per image at shape (1, 3, 448, 640)\n","\n","image 1/1 /content/drive/MyDrive/images/1102736_Hobo 1.jpg: 480x640 1 dog, 550.9ms\n","Speed: 6.2ms preprocess, 550.9ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n","\n","image 1/1 /content/drive/MyDrive/images/1107419_img 3.jpg: 480x640 1 dog, 1 couch, 526.4ms\n","Speed: 4.5ms preprocess, 526.4ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n","\n","image 1/1 /content/drive/MyDrive/images/1106219_oscar foster 5.jpg: 480x640 1 dog, 511.3ms\n","Speed: 4.0ms preprocess, 511.3ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n","\n","image 1/1 /content/drive/MyDrive/images/1109728_Brush 4 done.jpg: 480x640 1 dog, 495.6ms\n","Speed: 4.6ms preprocess, 495.6ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 640)\n","\n","image 1/1 /content/drive/MyDrive/images/1113210_img 1.jpg: 480x640 1 dog, 1 sports ball, 501.9ms\n","Speed: 3.4ms preprocess, 501.9ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n","\n","image 1/1 /content/drive/MyDrive/images/1111403_Defa 403 new 2.jpg: 480x640 1 dog, 1 potted plant, 522.3ms\n","Speed: 3.2ms preprocess, 522.3ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n","\n","image 1/1 /content/drive/MyDrive/images/1112490_poser.jpg: 480x640 1 dog, 527.0ms\n","Speed: 4.5ms preprocess, 527.0ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n","\n","image 1/1 /content/drive/MyDrive/images/1116441_tilly b.jpg: 480x640 1 dog, 493.8ms\n","Speed: 3.6ms preprocess, 493.8ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n","\n","image 1/1 /content/drive/MyDrive/images/1113210_IMG_8019.jpg: 480x640 1 dog, 544.3ms\n","Speed: 3.9ms preprocess, 544.3ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 640)\n","\n","image 1/1 /content/drive/MyDrive/images/1114449_Tobs.jpeg: 480x640 1 bench, 1 dog, 503.3ms\n","Speed: 3.8ms preprocess, 503.3ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n","\n","image 1/1 /content/drive/MyDrive/images/1120477_Toby 3.jpg: 448x640 1 person, 1 dog, 459.5ms\n","Speed: 3.2ms preprocess, 459.5ms inference, 1.8ms postprocess per image at shape (1, 3, 448, 640)\n","\n","image 1/1 /content/drive/MyDrive/images/1116896_1116896 Poppy.jpg: 448x640 1 dog, 476.2ms\n","Speed: 5.1ms preprocess, 476.2ms inference, 2.0ms postprocess per image at shape (1, 3, 448, 640)\n","\n","image 1/1 /content/drive/MyDrive/images/1119255_P2408349.jpg: 448x640 1 person, 1 traffic light, 1 dog, 1 baseball bat, 512.8ms\n","Speed: 4.2ms preprocess, 512.8ms inference, 1.8ms postprocess per image at shape (1, 3, 448, 640)\n","\n","image 1/1 /content/drive/MyDrive/images/1124725_20221218_130207000_iOS.jpg: 480x640 1 dog, 1 frisbee, 551.4ms\n","Speed: 4.5ms preprocess, 551.4ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n","\n","image 1/1 /content/drive/MyDrive/images/1122294_1122294 Benji 3.jpg: 448x640 1 dog, 1 sports ball, 523.7ms\n","Speed: 5.1ms preprocess, 523.7ms inference, 1.6ms postprocess per image at shape (1, 3, 448, 640)\n","\n","image 1/1 /content/drive/MyDrive/images/1124673_1124673 Ollie 4.jpg: 448x640 1 person, 1 dog, 545.7ms\n","Speed: 4.1ms preprocess, 545.7ms inference, 1.8ms postprocess per image at shape (1, 3, 448, 640)\n","\n","image 1/1 /content/drive/MyDrive/images/1126913_1126913 Milo.jpg: 448x640 1 dog, 729.1ms\n","Speed: 7.1ms preprocess, 729.1ms inference, 2.4ms postprocess per image at shape (1, 3, 448, 640)\n","\n","image 1/1 /content/drive/MyDrive/images/1127521_Phil5.jpg: 480x640 1 dog, 746.7ms\n","Speed: 5.7ms preprocess, 746.7ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n","\n","image 1/1 /content/drive/MyDrive/images/1124725_IMG_6149.jpg: 448x640 1 dog, 777.0ms\n","Speed: 6.0ms preprocess, 777.0ms inference, 2.5ms postprocess per image at shape (1, 3, 448, 640)\n","\n","image 1/1 /content/drive/MyDrive/images/1133008_Kuna3.jpg: 480x640 1 dog, 894.1ms\n","Speed: 7.2ms preprocess, 894.1ms inference, 3.2ms postprocess per image at shape (1, 3, 480, 640)\n","\n","image 1/1 /content/drive/MyDrive/images/1132302_Adam 3.jpg: 480x640 1 dog, 884.3ms\n","Speed: 9.2ms preprocess, 884.3ms inference, 2.4ms postprocess per image at shape (1, 3, 480, 640)\n","\n","image 1/1 /content/drive/MyDrive/images/1135196_IMG_7242.jpg: 448x640 1 dog, 510.8ms\n","Speed: 5.3ms preprocess, 510.8ms inference, 1.8ms postprocess per image at shape (1, 3, 448, 640)\n","\n","image 1/1 /content/drive/MyDrive/images/1135555_Dolly 1.jpg: 544x640 1 person, 1 dog, 601.6ms\n","Speed: 5.2ms preprocess, 601.6ms inference, 1.8ms postprocess per image at shape (1, 3, 544, 640)\n","\n","image 1/1 /content/drive/MyDrive/images/1135556_Dolly and Jolene 3.jpg: 448x640 1 dog, 9 sheeps, 499.3ms\n","Speed: 8.5ms preprocess, 499.3ms inference, 1.6ms postprocess per image at shape (1, 3, 448, 640)\n","\n","image 1/1 /content/drive/MyDrive/images/1137409_Bob 409 1.jpg: 448x640 1 dog, 504.4ms\n","Speed: 3.6ms preprocess, 504.4ms inference, 1.9ms postprocess per image at shape (1, 3, 448, 640)\n","\n","image 1/1 /content/drive/MyDrive/images/1136092_Rudi A Done.jpg: 480x640 1 dog, 490.7ms\n","Speed: 3.6ms preprocess, 490.7ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n","\n","image 1/1 /content/drive/MyDrive/images/1140676_Walter.jpg: 480x640 1 dog, 1 tie, 496.7ms\n","Speed: 6.0ms preprocess, 496.7ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n","\n","image 1/1 /content/drive/MyDrive/images/1141772_Glen 4.jpg: 480x640 1 dog, 479.4ms\n","Speed: 3.7ms preprocess, 479.4ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n","\n","image 1/1 /content/drive/MyDrive/images/1143371_adorable smol.jpg: 448x640 1 dog, 461.2ms\n","Speed: 5.0ms preprocess, 461.2ms inference, 1.3ms postprocess per image at shape (1, 3, 448, 640)\n","\n","image 1/1 /content/drive/MyDrive/images/1142732_Sephe 732 2.jpg: 448x640 1 dog, 453.8ms\n","Speed: 3.0ms preprocess, 453.8ms inference, 3.4ms postprocess per image at shape (1, 3, 448, 640)\n","\n","image 1/1 /content/drive/MyDrive/images/1145217_vincent 3.jpg: 448x640 1 person, 1 dog, 476.8ms\n","Speed: 2.9ms preprocess, 476.8ms inference, 1.6ms postprocess per image at shape (1, 3, 448, 640)\n","\n","image 1/1 /content/drive/MyDrive/images/1145318_Jess 2.jpg: 480x640 1 dog, 515.5ms\n","Speed: 5.0ms preprocess, 515.5ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 640)\n","\n","image 1/1 /content/drive/MyDrive/images/1145320_Todd 1 done.jpg: 480x640 1 dog, 491.5ms\n","Speed: 4.4ms preprocess, 491.5ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n","\n","image 1/1 /content/drive/MyDrive/images/1147700_Milo (7).jpg: 480x640 2 dogs, 472.1ms\n","Speed: 3.5ms preprocess, 472.1ms inference, 2.5ms postprocess per image at shape (1, 3, 480, 640)\n","\n","image 1/1 /content/drive/MyDrive/images/1147106_20221117_132335000_iOS.jpg: 480x640 1 dog, 488.1ms\n","Speed: 3.5ms preprocess, 488.1ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n","\n","image 1/1 /content/drive/MyDrive/images/1148878_Hetty done A (2).jpg: 480x640 1 dog, 1 couch, 495.1ms\n","Speed: 3.9ms preprocess, 495.1ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 640)\n","\n","image 1/1 /content/drive/MyDrive/images/1151414_sam 2.jpg: 448x640 1 person, 1 dog, 521.1ms\n","Speed: 4.7ms preprocess, 521.1ms inference, 1.7ms postprocess per image at shape (1, 3, 448, 640)\n","\n","image 1/1 /content/drive/MyDrive/images/1151414_sam 1.jpg: 448x640 1 dog, 479.8ms\n","Speed: 4.6ms preprocess, 479.8ms inference, 1.5ms postprocess per image at shape (1, 3, 448, 640)\n","\n","image 1/1 /content/drive/MyDrive/images/1150094_IMG_8677.jpg: 480x640 1 dog, 770.8ms\n","Speed: 5.1ms preprocess, 770.8ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n","\n","image 1/1 /content/drive/MyDrive/images/1150997_20221215_102303000_iOS 1.jpg: 480x640 2 cars, 1 dog, 775.9ms\n","Speed: 4.8ms preprocess, 775.9ms inference, 2.2ms postprocess per image at shape (1, 3, 480, 640)\n","\n","image 1/1 /content/drive/MyDrive/images/1150997_b1764afd-73d7-42fe-8d2b-b0ae683ba142.jpg: 640x608 1 dog, 965.5ms\n","Speed: 6.0ms preprocess, 965.5ms inference, 1.9ms postprocess per image at shape (1, 3, 640, 608)\n","\n","image 1/1 /content/drive/MyDrive/images/1154454_RUSSELL-JACK-CROSS-SEP-3.jpg: 448x640 1 person, 1 dog, 698.6ms\n","Speed: 4.6ms preprocess, 698.6ms inference, 2.2ms postprocess per image at shape (1, 3, 448, 640)\n","\n","image 1/1 /content/drive/MyDrive/images/1152252_emma daisy.jpg: 480x640 1 person, 1 bench, 1 dog, 786.7ms\n","Speed: 5.2ms preprocess, 786.7ms inference, 7.3ms postprocess per image at shape (1, 3, 480, 640)\n","\n","image 1/1 /content/drive/MyDrive/images/1152252_daisy c (3).jpg: 480x640 1 dog, 809.2ms\n","Speed: 6.6ms preprocess, 809.2ms inference, 5.1ms postprocess per image at shape (1, 3, 480, 640)\n","\n","image 1/1 /content/drive/MyDrive/images/1152252_daisy a (3).jpg: 480x640 1 dog, 631.5ms\n","Speed: 7.0ms preprocess, 631.5ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n","\n","image 1/1 /content/drive/MyDrive/images/1154821_Pixie 821 2.jpg: 480x640 1 person, 2 dogs, 1 potted plant, 565.7ms\n","Speed: 4.6ms preprocess, 565.7ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n","\n","image 1/1 /content/drive/MyDrive/images/1154454_RUSSELL-JACK-CROSS-SEP-8.jpg: 416x640 1 dog, 462.9ms\n","Speed: 3.9ms preprocess, 462.9ms inference, 1.5ms postprocess per image at shape (1, 3, 416, 640)\n","\n","image 1/1 /content/drive/MyDrive/images/1152252_daisy c (4).jpg: 480x640 1 dog, 566.9ms\n","Speed: 4.6ms preprocess, 566.9ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 640)\n","\n","image 1/1 /content/drive/MyDrive/images/1154096_TinyNEW001.jpg: 480x640 1 dog, 507.9ms\n","Speed: 4.7ms preprocess, 507.9ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n","\n","image 1/1 /content/drive/MyDrive/images/1154454_RUSSELL-JACK-CROSS-SEP-5.jpg: 448x640 1 person, 1 dog, 466.0ms\n","Speed: 3.6ms preprocess, 466.0ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n","\n","image 1/1 /content/drive/MyDrive/images/1154454_RUSSELL-JACK-CROSS-SEP-7.jpg: 448x640 1 person, 1 dog, 502.2ms\n","Speed: 3.5ms preprocess, 502.2ms inference, 1.7ms postprocess per image at shape (1, 3, 448, 640)\n","\n","image 1/1 /content/drive/MyDrive/images/1151414_sam 3.jpg: 448x640 1 dog, 496.4ms\n","Speed: 4.4ms preprocess, 496.4ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n","\n","image 1/1 /content/drive/MyDrive/images/1154454_RUSSELL-JACK-CROSS-SEP-2.jpg: 448x640 1 person, 1 dog, 447.2ms\n","Speed: 3.5ms preprocess, 447.2ms inference, 1.3ms postprocess per image at shape (1, 3, 448, 640)\n","\n","image 1/1 /content/drive/MyDrive/images/1152252_daisy a (4).jpg: 480x640 1 dog, 501.0ms\n","Speed: 3.6ms preprocess, 501.0ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n","\n","image 1/1 /content/drive/MyDrive/images/1154454_RUSSELL-JACK-CROSS-SEP.jpg: 448x640 1 dog, 490.6ms\n","Speed: 3.9ms preprocess, 490.6ms inference, 2.2ms postprocess per image at shape (1, 3, 448, 640)\n","\n","image 1/1 /content/drive/MyDrive/images/1154821_Pixie 821 3.jpg: 480x640 2 persons, 1 dog, 510.2ms\n","Speed: 4.9ms preprocess, 510.2ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n","\n","image 1/1 /content/drive/MyDrive/images/1154821_Pixie 821 1.jpg: 480x640 1 dog, 517.0ms\n","Speed: 3.2ms preprocess, 517.0ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n","\n","image 1/1 /content/drive/MyDrive/images/1155659_2.jpg: 448x640 1 dog, 1 sports ball, 474.9ms\n","Speed: 3.9ms preprocess, 474.9ms inference, 1.5ms postprocess per image at shape (1, 3, 448, 640)\n","\n","image 1/1 /content/drive/MyDrive/images/1156373_DAISY-LAB-OCT-3.jpg: 448x640 1 person, 1 dog, 1 sports ball, 489.4ms\n","Speed: 4.5ms preprocess, 489.4ms inference, 1.7ms postprocess per image at shape (1, 3, 448, 640)\n","\n","image 1/1 /content/drive/MyDrive/images/1156373_DAISY-LAB-OCT-9.jpg: 448x640 1 dog, 500.1ms\n","Speed: 3.6ms preprocess, 500.1ms inference, 1.6ms postprocess per image at shape (1, 3, 448, 640)\n","\n","image 1/1 /content/drive/MyDrive/images/1156356_phone pic.jpg: 480x640 1 dog, 1 frisbee, 543.3ms\n","Speed: 4.9ms preprocess, 543.3ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 640)\n","\n","image 1/1 /content/drive/MyDrive/images/1154821_Pixie 821 4.jpg: 448x640 1 dog, 1 bottle, 464.6ms\n","Speed: 3.4ms preprocess, 464.6ms inference, 1.3ms postprocess per image at shape (1, 3, 448, 640)\n","\n","image 1/1 /content/drive/MyDrive/images/1156356_with Nicola.jpg: 480x640 1 person, 1 bench, 1 dog, 736.6ms\n","Speed: 4.0ms preprocess, 736.6ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n","\n","image 1/1 /content/drive/MyDrive/images/1154890_Milo lurcher.jpg: 480x640 2 benchs, 1 dog, 790.9ms\n","Speed: 5.3ms preprocess, 790.9ms inference, 2.2ms postprocess per image at shape (1, 3, 480, 640)\n","\n","image 1/1 /content/drive/MyDrive/images/1156373_DAISY-LAB-OCT-4.jpg: 448x640 1 person, 2 dogs, 1 frisbee, 752.7ms\n","Speed: 5.4ms preprocess, 752.7ms inference, 2.2ms postprocess per image at shape (1, 3, 448, 640)\n","\n","image 1/1 /content/drive/MyDrive/images/1156373_DAISY-LAB-OCT-7.jpg: 448x640 1 person, 1 dog, 725.1ms\n","Speed: 5.8ms preprocess, 725.1ms inference, 2.3ms postprocess per image at shape (1, 3, 448, 640)\n","\n","image 1/1 /content/drive/MyDrive/images/1156356_run.jpg: 384x640 1 dog, 635.1ms\n","Speed: 6.6ms preprocess, 635.1ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n","\n","image 1/1 /content/drive/MyDrive/images/1156356_smile.jpg: 448x640 1 dog, 734.7ms\n","Speed: 4.4ms preprocess, 734.7ms inference, 1.7ms postprocess per image at shape (1, 3, 448, 640)\n","\n","image 1/1 /content/drive/MyDrive/images/1156373_DAISY-LAB-OCT-6.jpg: 448x640 1 person, 1 dog, 669.5ms\n","Speed: 4.5ms preprocess, 669.5ms inference, 1.7ms postprocess per image at shape (1, 3, 448, 640)\n","\n","image 1/1 /content/drive/MyDrive/images/1155659_1.jpg: 448x640 1 dog, 1 frisbee, 477.5ms\n","Speed: 4.2ms preprocess, 477.5ms inference, 1.6ms postprocess per image at shape (1, 3, 448, 640)\n","\n","image 1/1 /content/drive/MyDrive/images/1157961_Rocky (2).jpg: 480x640 1 dog, 11 books, 527.0ms\n","Speed: 3.7ms preprocess, 527.0ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n","\n","image 1/1 /content/drive/MyDrive/images/1157868_PXL_20221229_213313626.jpg: 512x640 1 dog, 1 couch, 1 bed, 541.3ms\n","Speed: 4.3ms preprocess, 541.3ms inference, 1.5ms postprocess per image at shape (1, 3, 512, 640)\n","\n","image 1/1 /content/drive/MyDrive/images/1157961_Rocky (4).jpg: 448x640 1 person, 1 dog, 1 couch, 512.7ms\n","Speed: 5.1ms preprocess, 512.7ms inference, 1.7ms postprocess per image at shape (1, 3, 448, 640)\n","\n","image 1/1 /content/drive/MyDrive/images/1157961_Rocky.jpg: 448x640 1 dog, 3 couchs, 488.1ms\n","Speed: 4.1ms preprocess, 488.1ms inference, 1.6ms postprocess per image at shape (1, 3, 448, 640)\n","\n","image 1/1 /content/drive/MyDrive/images/1158779_expecting you.jpg: 480x640 1 dog, 1 couch, 514.3ms\n","Speed: 5.0ms preprocess, 514.3ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n","\n","image 1/1 /content/drive/MyDrive/images/1157166_eddy jrt (3) (Small).jpg: 448x640 1 dog, 494.6ms\n","Speed: 3.5ms preprocess, 494.6ms inference, 1.7ms postprocess per image at shape (1, 3, 448, 640)\n","\n","image 1/1 /content/drive/MyDrive/images/1157961_Rocky (3).jpg: 448x640 1 bear, 449.5ms\n","Speed: 4.1ms preprocess, 449.5ms inference, 1.5ms postprocess per image at shape (1, 3, 448, 640)\n","\n","image 1/1 /content/drive/MyDrive/images/1157166_eddy jrt (2) (Small).jpg: 448x640 1 dog, 509.4ms\n","Speed: 3.8ms preprocess, 509.4ms inference, 1.7ms postprocess per image at shape (1, 3, 448, 640)\n","\n","image 1/1 /content/drive/MyDrive/images/1157961_Rocky_11.jpg: 480x640 1 dog, 1 cup, 559.9ms\n","Speed: 3.4ms preprocess, 559.9ms inference, 2.2ms postprocess per image at shape (1, 3, 480, 640)\n","\n","image 1/1 /content/drive/MyDrive/images/1157166_eddy jrt (1) (Small).jpg: 448x640 1 dog, 524.6ms\n","Speed: 4.3ms preprocess, 524.6ms inference, 1.8ms postprocess per image at shape (1, 3, 448, 640)\n","\n","image 1/1 /content/drive/MyDrive/images/1158779_image00010.jpeg: 480x640 1 dog, 500.9ms\n","Speed: 4.9ms preprocess, 500.9ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n","\n","image 1/1 /content/drive/MyDrive/images/1156373_DAISY-LAB-OCT.jpg: 448x640 1 person, 1 dog, 1 handbag, 1 sports ball, 1 bottle, 490.3ms\n","Speed: 3.9ms preprocess, 490.3ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n","\n","image 1/1 /content/drive/MyDrive/images/1157961_Rocky (5).jpg: 480x640 1 dog, 483.9ms\n","Speed: 3.0ms preprocess, 483.9ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n","\n","image 1/1 /content/drive/MyDrive/images/1158779_new2.jpg: 480x640 1 dog, 531.4ms\n","Speed: 3.6ms preprocess, 531.4ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 640)\n","\n","image 1/1 /content/drive/MyDrive/images/1158779_new3.jpg: 480x640 1 dog, 1 teddy bear, 670.2ms\n","Speed: 4.7ms preprocess, 670.2ms inference, 2.6ms postprocess per image at shape (1, 3, 480, 640)\n","\n","image 1/1 /content/drive/MyDrive/images/1159737_BUSTER-TERRIER-MAR-2.jpg: 448x640 1 person, 1 dog, 716.8ms\n","Speed: 4.9ms preprocess, 716.8ms inference, 1.7ms postprocess per image at shape (1, 3, 448, 640)\n","\n","image 1/1 /content/drive/MyDrive/images/1158779_Oliver.jpg: 480x640 1 dog, 1 potted plant, 749.0ms\n","Speed: 7.7ms preprocess, 749.0ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n","\n","image 1/1 /content/drive/MyDrive/images/1036013_rosie b.jpg: 480x640 1 dog, 1 bed, 769.3ms\n","Speed: 5.0ms preprocess, 769.3ms inference, 2.1ms postprocess per image at shape (1, 3, 480, 640)\n","\n","image 1/1 /content/drive/MyDrive/images/1041720_IMG_5954 (1).jpg: 448x640 1 dog, 732.0ms\n","Speed: 7.5ms preprocess, 732.0ms inference, 4.0ms postprocess per image at shape (1, 3, 448, 640)\n","\n","image 1/1 /content/drive/MyDrive/images/1057077_IMG_5179.jpg: 448x640 1 person, 1 dog, 742.5ms\n","Speed: 4.7ms preprocess, 742.5ms inference, 2.7ms postprocess per image at shape (1, 3, 448, 640)\n","\n","image 1/1 /content/drive/MyDrive/images/1052012_1052012 - Boots (16)-min.jpg: 448x640 1 dog, 637.2ms\n","Speed: 5.9ms preprocess, 637.2ms inference, 2.0ms postprocess per image at shape (1, 3, 448, 640)\n","\n","image 1/1 /content/drive/MyDrive/images/1060362_barney a.jpg: 480x640 1 dog, 517.7ms\n","Speed: 4.4ms preprocess, 517.7ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 640)\n","\n","image 1/1 /content/drive/MyDrive/images/1066556_20230405_151737002_iOS.jpg: 480x640 1 dog, 488.4ms\n","Speed: 3.2ms preprocess, 488.4ms inference, 2.1ms postprocess per image at shape (1, 3, 480, 640)\n","\n","image 1/1 /content/drive/MyDrive/images/1060362_sarah barney 4.3.jpg: 480x640 1 person, 1 dog, 472.6ms\n","Speed: 3.6ms preprocess, 472.6ms inference, 2.7ms postprocess per image at shape (1, 3, 480, 640)\n","\n","image 1/1 /content/drive/MyDrive/images/1077675_20220816_181255000_iOS.jpg: 480x640 1 dog, 483.3ms\n","Speed: 3.5ms preprocess, 483.3ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n","\n","image 1/1 /content/drive/MyDrive/images/1078112_Flo (15).jpg: 480x640 1 dog, 513.5ms\n","Speed: 3.5ms preprocess, 513.5ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n","Results saved to image_labels_10.csv\n"]}]},{"cell_type":"markdown","source":["Split the image folder into subsets for easier processing"],"metadata":{"id":"JZNunLzAeeBO"}},{"cell_type":"code","source":["import os\n","import shutil\n","import math\n","\n","\n","original_folder_path = '#/content/drive/My Drive/images'\n","destination_base_path = '#/content/drive/My Drive/split_images'\n","\n","# Create the base directory for split folders\n","if not os.path.exists(destination_base_path):\n","    os.makedirs(destination_base_path)\n","\n","# List all image files in the original folder\n","all_images = [f for f in os.listdir(original_folder_path) if os.path.isfile(os.path.join(original_folder_path, f))]\n","\n","# Calculate the number of images per folder\n","num_images = len(all_images)\n","num_folders = 9\n","images_per_folder = math.ceil(num_images / num_folders)\n","\n","print(f\"Images successfully split into {num_folders} folders.\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"L3KGdLf-rMcw","executionInfo":{"status":"ok","timestamp":1720515225349,"user_tz":-60,"elapsed":55734,"user":{"displayName":"Jack Wardle","userId":"17218716300059501347"}},"outputId":"a73d003a-a6b0-4229-db6f-217079fb0a80"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Images successfully split into 9 folders.\n"]}]},{"cell_type":"markdown","source":["Used to validate the split sizes of the folders"],"metadata":{"id":"1pYIoyoVeoGh"}},{"cell_type":"code","source":["import os\n","\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","original_folder_path = '/content/drive/My Drive/images'\n","split_base_path = '/content/drive/My Drive/split_images'\n","\n","# List all image files in the original folder\n","original_images = [f for f in os.listdir(original_folder_path) if os.path.isfile(os.path.join(original_folder_path, f))]\n","original_image_count = len(original_images)\n","\n","print(f\"Number of images in the original folder: {original_image_count}\")\n","\n","# Check the number of images in each split subfolder\n","total_split_images = 0\n","\n","for i in range(9):\n","    subfolder_path = os.path.join(split_base_path, f'subfolder_{i+1}')\n","    if os.path.exists(subfolder_path):\n","        split_images = [f for f in os.listdir(subfolder_path) if os.path.isfile(os.path.join(subfolder_path, f))]\n","        split_image_count = len(split_images)\n","        total_split_images += split_image_count\n","        print(f\"Number of images in {subfolder_path}: {split_image_count}\")\n","    else:\n","        print(f\"{subfolder_path} does not exist.\")\n","\n","print(f\"Total number of images in all split subfolders: {total_split_images}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sy4asmAE4Gv4","executionInfo":{"status":"ok","timestamp":1720518685013,"user_tz":-60,"elapsed":4348,"user":{"displayName":"Jack Wardle","userId":"17218716300059501347"}},"outputId":"59411107-8bf9-4019-f553-ac11beaffbb7"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","Number of images in the original folder: 108\n","Number of images in /content/drive/My Drive/split_images/subfolder_1: 1438\n","Number of images in /content/drive/My Drive/split_images/subfolder_2: 1438\n","Number of images in /content/drive/My Drive/split_images/subfolder_3: 1438\n","Number of images in /content/drive/My Drive/split_images/subfolder_4: 1438\n","Number of images in /content/drive/My Drive/split_images/subfolder_5: 1438\n","Number of images in /content/drive/My Drive/split_images/subfolder_6: 1438\n","Number of images in /content/drive/My Drive/split_images/subfolder_7: 1438\n","Number of images in /content/drive/My Drive/split_images/subfolder_8: 1438\n","Number of images in /content/drive/My Drive/split_images/subfolder_9: 1433\n","Total number of images in all split subfolders: 12937\n"]}]},{"cell_type":"markdown","source":["Investigation into scene classification"],"metadata":{"id":"EYPBsuzKpyre"}},{"cell_type":"code","source":["!pip install torch torchvision pandas"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"e19zJ7bdp2ql","executionInfo":{"status":"ok","timestamp":1720531279421,"user_tz":-60,"elapsed":12585,"user":{"displayName":"Jack Wardle","userId":"17218716300059501347"}},"outputId":"794f8663-eaf5-44bc-ec38-2fd1411630e0"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.3.0+cu121)\n","Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.18.0+cu121)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.0.3)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.15.4)\n","Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12.1)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.3)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2023.6.0)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n","Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch) (8.9.2.26)\n","Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.3.1)\n","Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch) (11.0.2.54)\n","Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch) (10.3.2.106)\n","Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch) (11.4.5.107)\n","Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.0.106)\n","Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.10/dist-packages (from torch) (2.20.5)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n","Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.3.0)\n","Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch) (12.5.82)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.25.2)\n","Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (9.4.0)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2023.4)\n","Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.1)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n","Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n"]}]},{"cell_type":"code","source":["import torch\n","from torchvision import models, transforms\n","\n","# Download the Places365 category list and scene attribute files\n","!wget http://places2.csail.mit.edu/models_places365/categories_places365.txt\n","!wget http://places2.csail.mit.edu/models_places365/IO_places365.txt\n","\n","# Download the ResNet18 model pre-trained on Places365\n","!wget http://places2.csail.mit.edu/models_places365/resnet18_places365.pth.tar\n","\n","# Define the model and load the pre-trained weights\n","model = models.resnet18(num_classes=365)\n","checkpoint = torch.load('resnet18_places365.pth.tar', map_location=torch.device('cpu'))\n","model.load_state_dict({k.replace('module.', ''): v for k, v in checkpoint['state_dict'].items()})\n","model.eval()\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0A8dW0SNqMnH","executionInfo":{"status":"ok","timestamp":1720531353522,"user_tz":-60,"elapsed":2748,"user":{"displayName":"Jack Wardle","userId":"17218716300059501347"}},"outputId":"b0f868c2-3801-4e16-d982-05c746d31614"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["--2024-07-09 13:22:31--  http://places2.csail.mit.edu/models_places365/categories_places365.txt\n","Resolving places2.csail.mit.edu (places2.csail.mit.edu)... 128.52.132.120\n","Connecting to places2.csail.mit.edu (places2.csail.mit.edu)|128.52.132.120|:80... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 6833 (6.7K) [text/plain]\n","Saving to: ‘categories_places365.txt’\n","\n","categories_places36 100%[===================>]   6.67K  --.-KB/s    in 0s      \n","\n","2024-07-09 13:22:31 (321 MB/s) - ‘categories_places365.txt’ saved [6833/6833]\n","\n","--2024-07-09 13:22:31--  http://places2.csail.mit.edu/models_places365/IO_places365.txt\n","Resolving places2.csail.mit.edu (places2.csail.mit.edu)... 128.52.132.120\n","Connecting to places2.csail.mit.edu (places2.csail.mit.edu)|128.52.132.120|:80... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 6214 (6.1K) [text/plain]\n","Saving to: ‘IO_places365.txt’\n","\n","IO_places365.txt    100%[===================>]   6.07K  --.-KB/s    in 0s      \n","\n","2024-07-09 13:22:31 (189 MB/s) - ‘IO_places365.txt’ saved [6214/6214]\n","\n","--2024-07-09 13:22:31--  http://places2.csail.mit.edu/models_places365/resnet18_places365.pth.tar\n","Resolving places2.csail.mit.edu (places2.csail.mit.edu)... 128.52.132.120\n","Connecting to places2.csail.mit.edu (places2.csail.mit.edu)|128.52.132.120|:80... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 45506139 (43M) [application/x-tar]\n","Saving to: ‘resnet18_places365.pth.tar’\n","\n","resnet18_places365. 100%[===================>]  43.40M  30.9MB/s    in 1.4s    \n","\n","2024-07-09 13:22:33 (30.9 MB/s) - ‘resnet18_places365.pth.tar’ saved [45506139/45506139]\n","\n"]},{"output_type":"execute_result","data":{"text/plain":["ResNet(\n","  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n","  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","  (relu): ReLU(inplace=True)\n","  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n","  (layer1): Sequential(\n","    (0): BasicBlock(\n","      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","    (1): BasicBlock(\n","      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","  )\n","  (layer2): Sequential(\n","    (0): BasicBlock(\n","      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (downsample): Sequential(\n","        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","    )\n","    (1): BasicBlock(\n","      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","  )\n","  (layer3): Sequential(\n","    (0): BasicBlock(\n","      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (downsample): Sequential(\n","        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","    )\n","    (1): BasicBlock(\n","      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","  )\n","  (layer4): Sequential(\n","    (0): BasicBlock(\n","      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (downsample): Sequential(\n","        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","    )\n","    (1): BasicBlock(\n","      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","  )\n","  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n","  (fc): Linear(in_features=512, out_features=365, bias=True)\n",")"]},"metadata":{},"execution_count":20}]},{"cell_type":"code","source":["import os\n","from PIL import Image\n","import pandas as pd\n","\n","image_folder_path = '/content/drive/My Drive/split_images/subfolder_9'\n","\n","# Define image transformation\n","transform = transforms.Compose([\n","    transforms.Resize((256, 256)),\n","    transforms.CenterCrop(224),\n","    transforms.ToTensor(),\n","    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n","])\n","\n","# Load images\n","def load_images_from_folder(folder):\n","    images = []\n","    image_names = []\n","    for filename in os.listdir(folder):\n","        if filename.endswith(('.jpg', '.jpeg', '.png')):\n","            img_path = os.path.join(folder, filename)\n","            image = Image.open(img_path).convert('RGB')\n","            image = transform(image)\n","            images.append(image)\n","            image_names.append(filename)\n","    return images, image_names\n","\n","images, image_names = load_images_from_folder(image_folder_path)\n"],"metadata":{"id":"dUcC5CKdqOcS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Load the category and indoor/outdoor labels\n","categories = [line.strip().split(' ')[0][3:] for line in open('categories_places365.txt')]\n","with open('IO_places365.txt') as f:\n","    lines = f.readlines()\n","    labels_IO = [int(line.rstrip().split()[-1]) -1 for line in lines]\n","\n","# Function to classify an image\n","def classify_image(image, model):\n","    image = image.unsqueeze(0)  # Add batch dimension\n","    with torch.no_grad():\n","        logit = model(image)\n","    h_x = torch.nn.functional.softmax(logit, 1).data.squeeze()\n","    probs, idx = h_x.sort(0, True)\n","    return 'outdoor' if labels_IO[idx[0]] == 1 else 'indoor'\n","\n","# Classify all images and save results to a CSV\n","results = []\n","for image, image_name in zip(images, image_names):\n","    classification = classify_image(image, model)\n","    results.append([image_name, classification])\n","\n","df = pd.DataFrame(results, columns=['ImageName', 'Classification'])\n","output_path = '/content/drive/My Drive/Scene/image_classifications_9.csv'\n","df.to_csv(output_path, index=False)\n","\n","print(f\"Results saved to {output_path}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"AFLRi6jkqh_u","executionInfo":{"status":"ok","timestamp":1720538629352,"user_tz":-60,"elapsed":173714,"user":{"displayName":"Jack Wardle","userId":"17218716300059501347"}},"outputId":"12c03a99-4e63-43aa-b7b3-22e922232570"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Results saved to /content/drive/My Drive/Scene/image_classifications_9.csv\n"]}]}]}